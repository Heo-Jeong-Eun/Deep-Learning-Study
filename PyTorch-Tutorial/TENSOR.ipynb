{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc1a74cc-f6d2-4196-8096-8474a89ac61f",
   "metadata": {},
   "source": [
    "# TENSOR\n",
    "\n",
    "**텐서(tensor)는 배열(array)이나 행렬(matrix)과 매우 유사한 특수한 자료구조**이다. <br>\n",
    "PyTorch에서는 텐서를 사용하여 모델의 입력(input)과 출력(output), 그리고 모델의 매개변수들을 부호화(encode)한다. <br>\n",
    "텐서는 GPU나 다른 하드웨어 가속기에서 실행할 수 있다는 점만 제외하면 NumPy 의 ndarray와 유사하다. <br>\n",
    "실제로 텐서와 NumPy 배열(array)은 종종 동일한 내부(underly) 메모리를 공유할 수 있어 데이터를 복사할 필요가 없다. <br> \n",
    "(NumPy 변환(Bridge) 참고) 텐서는 또한 (Autograd 장에서 살펴볼) 자동 미분(automatic differentiation)에 최적화되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fc9811f-45af-4ece-9672-9dd2d1cadb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08812018-9c7e-4f03-b7ea-617701c61b9f",
   "metadata": {},
   "source": [
    "## 텐서(tensor) 초기화\n",
    "\n",
    "텐서는 여러 방법으로 초기화 할 수 있다. <br>\n",
    "\n",
    "### 데이터로부터 직접(directly) 생성하기\n",
    "\n",
    "데이터로부터 직접 텐서를 생성할 수 있다. 데이터의 자료형(data type)은 자동으로 유추한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02b1dbbe-d62b-43cc-8e87-d6efffbd9c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7767849b-79d7-4938-aabe-867ce879973e",
   "metadata": {},
   "source": [
    "### NumPy 배열로부터 생성하기\n",
    "\n",
    "텐서는 NumPy 배열로 생성할 수 있다. (그 반대도 가능하다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aa7bdf2-6467-4d2c-b23d-30a09181fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80157957-37e4-4bac-b78a-89ebba75e53c",
   "metadata": {},
   "source": [
    "### 다른 텐서로부터 생성하기\n",
    "\n",
    "명시적으로 재정의(override)하지 않는다면, 인자로 주어진 텐서의 속성(모양(shape), 자료형(datatype))을 유지한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f4d8199-b797-49dd-baf3-9e2059ad9467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor : \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor : \n",
      " tensor([[0.0605, 0.9094],\n",
      "        [0.2040, 0.3228]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data) # x_data의 속성을 유지한다.\n",
    "print(f\"Ones Tensor : \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype = torch.float) # x_data의 속성을 덮어쓴다.\n",
    "print(f\"Random Tensor : \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa46a20-71e9-4ffb-83bb-4d8baad9f3bc",
   "metadata": {},
   "source": [
    "### 무작위(random) 또는 상수(constant) 값을 사용하기\n",
    "\n",
    "shape은 텐서의 차원(dimension)을 나타내는 튜플(tuple)로, 아래 함수들에서는 출력 텐서의 차원을 결정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "315e9d04-569a-418e-9307-2d6bbf4c3710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor : \n",
      " tensor([[0.4931, 0.4607, 0.9558],\n",
      "        [0.7708, 0.7916, 0.3249]]) \n",
      "\n",
      "Ones Tensor : \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor : \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2, 3, )\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor : \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor : \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor : \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5188d8b5-607e-4b6a-a20b-9ec2bc7f3e1a",
   "metadata": {},
   "source": [
    "## 텐서의 속성(Attribute)\n",
    "\n",
    "텐서의 속성은 텐서의 모양(shape), 자료형(datatype) 및 어느 장치에 저장되는지를 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3d7a595-8502-47f6-bbed-a9cfcdf90af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor : torch.Size([3, 4])\n",
      "Datatype of tensor : torch.float32\n",
      "Device tensor is stored on : cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Shape of tensor : {tensor.shape}\")\n",
    "print(f\"Datatype of tensor : {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on : {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2355a75d-4bbd-4447-8183-f5bade897a0d",
   "metadata": {},
   "source": [
    "## 텐서 연산(Operation)\n",
    "\n",
    "전치(transposing), 인덱싱(indexing), 슬라이싱(slicing), 수학 계산, 선형 대수, 임의 샘플링(random sampling) 등, 100가지 이상의 텐서 연산들이 있다. <br>\n",
    "각 연산들은 (일반적으로 CPU보다 빠른) GPU에서 실행할 수 있습니다. Colab을 사용한다면, Edit > Notebook Settings 에서 GPU를 할당할 수 있다. <br>\n",
    "기본적으로 텐서는 CPU에 생성된다. <br>\n",
    ".to 메소드를 사용하면 (GPU의 가용성(availability)을 확인한 뒤) GPU로 텐서를 명시적으로 이동할 수 있다. <br>\n",
    "장치들 간에 큰 텐서들을 복사하는 것은 시간과 메모리 측면에서 비용이 많이든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8649c6c7-c190-40af-8631-d87f75dec802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU가 존재하면 텐서를 이동\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38c5d18-3636-4c8c-a027-1407262911d3",
   "metadata": {},
   "source": [
    "### NumPy식의 표준 인덱싱과 슬라이싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99f0837b-ca6f-4ed5-bce6-d8ce17eecb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row : tensor([1., 1., 1., 1.])\n",
      "First column : tensor([1., 1., 1., 1.])\n",
      "Last column : tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "print(f\"First row : {tensor[0]}\")\n",
    "print(f\"First column : {tensor[:, 0]}\")\n",
    "print(f\"Last column : {tensor[..., -1]}\")\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5799ff31-5046-4b0f-8b0e-739b81f0f29e",
   "metadata": {},
   "source": [
    "### 텐서 합치기 \n",
    "torch.cat을 사용하여 주어진 차원에 따라 일련의 텐서를 연결할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61b81880-26db-453a-a539-75634c99d9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim = 1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fe6c00-cd52-4a7e-a1a3-0b1e9d0b24de",
   "metadata": {},
   "source": [
    "### 산술 연산(Arithmetic operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b29fa3e-bbf1-438b-a541-0fe451180942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 두 텐서 간의 행렬 곱(matrix multiplication)을 계산한다. y1, y2, y3은 모두 같은 값을 갖는다.\n",
    "# ``tensor.T``는 텐서의 전치(transpose)를 반환한다.\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(y1)\n",
    "torch.matmul(tensor, tensor.T, out = y3)\n",
    "\n",
    "\n",
    "# 요소별 곱(element-wise product)을 계산한다. z1, z2, z3는 모두 같은 값을 갖는다.\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out = z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdfcc4d-cc4f-466f-bf3f-ed67fccadfff",
   "metadata": {},
   "source": [
    "### 단일-요소(single-element) 텐서\n",
    "\n",
    "텐서의 모든 값을 하나로 집계(aggregate)하여 요소가 하나인 텐서의 경우, item()을 사용하여 Python 숫자 값으로 변환할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "201b2e86-5d33-437b-84ee-e0cc40bf5167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607d7807-ea3a-472d-8c92-1a60f1a5de1f",
   "metadata": {},
   "source": [
    "### 바꿔치기(in-place) 연산\n",
    "\n",
    "연산 결과를 피연산자(operand)에 저장하는 연산을 바꿔치기 연산이라고 부르며, _ 접미사를 갖는다. <br>\n",
    "예를 들어: x.copy_(y) 나 x.t_() 는 x 를 변경한다. <br>\n",
    "\n",
    "바꿔치기 연산은 메모리를 일부 절약하지만, 기록(history)이 즉시 삭제되어 도함수(derivative) 계산에 문제가 발생할 수 있다. 따라서 사용을 권장하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b49773c5-d7d6-4b31-bd4e-8c41d7c79c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{tensor} \\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5c9592-5606-4160-982a-2423749d4bcf",
   "metadata": {},
   "source": [
    "## NumPy 변환(Bridge)\n",
    "\n",
    "CPU 상의 텐서와 NumPy 배열은 메모리 공간을 공유하기 때문에, 하나를 변경하면 다른 하나도 변경된다.\n",
    "\n",
    "### 텐서를 NumPy 배열로 변환하기\n",
    "\n",
    "텐서의 변경 사항이 NumPy 배열에 반영된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad0c4d76-d35c-4737-982a-c35324d76c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t : tensor([1., 1., 1., 1., 1.])\n",
      "n : [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t : {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n : {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9db05e-022c-4a89-9aae-74548ce73824",
   "metadata": {},
   "source": [
    "### NumPy 배열을 텐서로 변환하기\n",
    "\n",
    "NumPy 배열의 변경 사항이 텐서에 반영된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5459e80b-ce06-4b91-90c1-57ac15c00d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t : tensor([2., 2., 2., 2., 2.])\n",
      "n : [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "np.add(n, 1, out = n)\n",
    "print(f\"t : {t}\")\n",
    "print(f\"n : {n}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
