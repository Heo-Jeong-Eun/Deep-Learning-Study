{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPuC46sKY3vBVORb2VQ6fE1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 모델 매개변수 최적화하기\n","##### 모델과 데이터가 준비되어 있으니, 데이터에 매개변수를 최적화하여 모델을 학습하고, 검증하고, 테스트할 차례이다.\n","##### 모델을 학습하는 과정은 반복적인 과정이다. 각 반복 단계에서 모델은 출력을 추측하고, 추측과 정답 사이의 오류를 계산하고, 매개변수에 대한 오류의 도함수를 수집한 뒤, 경사하강법을 사용하여 이 파라미터를 최적화한다."],"metadata":{"id":"pleRJ7T_uoE1"}},{"cell_type":"markdown","source":["## 기본(Pre-requisite) 코드"],"metadata":{"id":"UN-nTUgIwFo1"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OaDxJCqQukxV","executionInfo":{"status":"ok","timestamp":1702531091985,"user_tz":-540,"elapsed":18600,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"1c96778f-c4b7-4d48-eec8-42b5c718889d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 26421880/26421880 [00:01<00:00, 15243425.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 29515/29515 [00:00<00:00, 267286.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4422102/4422102 [00:00<00:00, 5006014.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 5148/5148 [00:00<00:00, 16495245.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n","\n"]}],"source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","\n","training_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform=ToTensor()\n",")\n","\n","test_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform=ToTensor()\n",")\n","\n","train_dataloader = DataLoader(training_data, batch_size=64)\n","test_dataloader = DataLoader(test_data, batch_size=64)\n","\n","class NeuralNetwork(nn.Module):\n","    def __init__(self):\n","        super(NeuralNetwork, self).__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(28*28, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 10),\n","        )\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        logits = self.linear_relu_stack(x)\n","        return logits\n","\n","model = NeuralNetwork()"]},{"cell_type":"markdown","source":["## 하이퍼파라미터\n","##### 하이퍼파라미터는 모델 최적화 과정을 제어할 수 있는 조절 가능한 매개변수이다. 서로 다른 하이퍼파라미터 값은 모델 학습과 수렴율에 영향을 미칠 수 있다.\n","* 에폭 수 - 데이터셋을 반복하는 횟수\n","* 배치 크기 - 매개변수가 갱신되기 전 신경망을 통해 전파된 데이터 샘플의 수\n","* 학습률 - 각 배치/에폭에서 모델의 매개변수를 조절하는 비율. 값이 작을수록 학습 속도가 느려지고, 값이 크면 학습 중 예측할 수 없는 동작이 발생할 수 있다."],"metadata":{"id":"ZZBPNHYrwso0"}},{"cell_type":"code","source":["learning_rate = 1e-3\n","batch_size = 64\n","epochs = 5"],"metadata":{"id":"BybBDAawwmUT","executionInfo":{"status":"ok","timestamp":1702531306607,"user_tz":-540,"elapsed":8,"user":{"displayName":"천준석","userId":"06369588489203998973"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## 최적화 단계(Optimization Loop)\n","##### 하이퍼파라미터를 설정한 뒤에는 최적화 단계를 통해 모델을 학습하고 최적화할 수 있다. 최적화 단계의 각 반복을 에폭이라 한다.\n","##### 하나의 에폭은 아래와 같이 두 부분으로 구성된다.\n","* 학습 단계 - 학습용 데이터셋을 반복하고 최적의 매개변수로 수렴한다.\n","* 검증/테스트 단계 - 모델 성능이 개선되고 있는지를 확인하기 위해  테스트 데이터셋을 반복한다."],"metadata":{"id":"k3pCHc-bxhnn"}},{"cell_type":"markdown","source":["### 손실 함수(loss function)\n","##### 학습용 데이터를 제공하면, 학습되지 않은 신경망은 정답을 제공하지 않을 확률이 높다.\n","##### **손실 함수**는 예측 값과 실제 값 사이의 틀린 정도를 측정하며, 학습 중에 이 값을 최소화할려고 한다. 주어진 데이터 샘플을 입력으로 계산한 예측과 정답을 비교하여 손실(loss)를 계산한다."],"metadata":{"id":"Pcfe5x8vzIWi"}},{"cell_type":"markdown","source":["##### 일반적인 손실함수에는 회귀 문제에 사용하는 nn.MSELoss(평균 제곱 오차)나 분류에 사용하는 nn.NLLLoss(음의 로그 우도), 그리고 nn.LogSoftmax 와 nn.NLLLoss를 합친 nn.CrossEntropyLoss 등이 있다.\n","##### 모델의 출력 logits을 nn.CrossEntropyLoss에 전달하여 logits을 정규화하고 예측 오류를 계산한다."],"metadata":{"id":"MYg-HIXgzflL"}},{"cell_type":"code","source":["# 손실 함수를 초기화\n","loss_fn = nn.CrossEntropyLoss()"],"metadata":{"id":"DIPXqAYuxhIh","executionInfo":{"status":"ok","timestamp":1702531955335,"user_tz":-540,"elapsed":9,"user":{"displayName":"천준석","userId":"06369588489203998973"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["### 옵티마이저(Optimizer)\n","##### 최적화는 각 학습 단계에서 모델의 오류를 줄이기 위해 모델 매개변수를 조정하는 과정이다.\n","##### 최적화 알고리즘은 이 과정(=각 학습 단계에서 모델의 오류를 줄이기 위해 모델 매개변수를 조정하는)이 수행되는 방식을 정의한다.\n","##### 모든 최적화 로직은 optimizer 객체에 캡슐화된다. 여기서는 SGD 옵티마이저를 사용하고 있으며, PyTorch에는 ADAM, RMSProp과 같은 다른 종류의 모델과 데이터에서 더 잘 동작하는 다양한 옵티마이저가 있다."],"metadata":{"id":"wFZeNXat0AYx"}},{"cell_type":"code","source":["# 학습할려는 모델의 매개변수와 학습률 하이퍼파라미터를 등록하여 옵티마이저를 초기화\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"],"metadata":{"id":"LiwziPbyz_bu","executionInfo":{"status":"ok","timestamp":1702532187254,"user_tz":-540,"elapsed":371,"user":{"displayName":"천준석","userId":"06369588489203998973"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["##### 학습 단계에서 최적화는 아래와 같이 3단계로 이뤄진다.\n","* optimizer.zero_grad()를 호출하여 모델 매개변수의 변화도를 재설정한다. 기본적으로 변화도는 더해지기 때문에 중복 계산을 막기 위해 반복할 때마다 명시적으로 0으로 설정한다.\n","* loss.backward()를 호출하여 예측 손실을 역전파한다. PyTorch는 각 매개변수에 대한 손실의 변화도를 저장한다.\n","* 변화도를 계산한 뒤에는 optimizer.step()을 호출하여 역전파 단계에서 수집된 변화도로 매개변수를 조정한다."],"metadata":{"id":"8oZNVwFm05Mt"}},{"cell_type":"markdown","source":["## 전체 구현\n","##### 최적화 코드를 반복하여 수행하는 train_loop와 테스트 데이터로 모델의 성능을 측정하는 test_loop로 정의"],"metadata":{"id":"FlUURNKe1lVr"}},{"cell_type":"code","source":["def train_loop(dataloader, model, loss_fn, optimizer):\n","  size = len(dataloader.dataset)\n","  for batch, (X,y) in enumerate(dataloader):\n","    # 예측과 loss 계산\n","    pred = model(X)\n","    loss = loss_fn(pred, y)\n","\n","    # 역전파\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    if batch % 100 == 0:\n","      loss, current = loss.item(), (batch + 1) * len(X)\n","      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n","\n","def test_loop(dataloader, model, loss_fn):\n","  size = len(dataloader.dataset)\n","  num_batches = len(dataloader)\n","  test_loss, correct = 0, 0\n","\n","  # 단순 순전파 계산을 위해 torch.no_grad()\n","  with torch.no_grad():\n","    for X, y in dataloader:\n","      pred = model(X)\n","      test_loss += loss_fn(pred, y).item()\n","      # 최대값을 가지는 인덱스 = y 요소의 합\n","      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","  test_loss /= num_batches\n","  correct /= size\n","  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"],"metadata":{"id":"dHNIa0EU04Ex","executionInfo":{"status":"ok","timestamp":1702532647209,"user_tz":-540,"elapsed":7,"user":{"displayName":"천준석","userId":"06369588489203998973"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["##### 손실 함수와 옵티마이저를 초기화하고 train_loss 와 test_loss에 전달한다."],"metadata":{"id":"GSyc1Rm02_D6"}},{"cell_type":"code","source":["loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","epochs = 10\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train_loop(train_dataloader, model, loss_fn, optimizer)\n","    test_loop(test_dataloader, model, loss_fn)\n","print(\"Done!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D_SmWdZr2oV2","executionInfo":{"status":"ok","timestamp":1702533022652,"user_tz":-540,"elapsed":158219,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"27cc13f0-1fdc-4c28-ca0e-7171790c6e37"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1\n","-------------------------------\n","loss: 2.291444  [   64/60000]\n","loss: 2.280236  [ 6464/60000]\n","loss: 2.260664  [12864/60000]\n","loss: 2.265734  [19264/60000]\n","loss: 2.242474  [25664/60000]\n","loss: 2.217055  [32064/60000]\n","loss: 2.223252  [38464/60000]\n","loss: 2.184766  [44864/60000]\n","loss: 2.188356  [51264/60000]\n","loss: 2.158250  [57664/60000]\n","Test Error: \n"," Accuracy: 43.7%, Avg loss: 2.148204 \n","\n","Epoch 2\n","-------------------------------\n","loss: 2.163132  [   64/60000]\n","loss: 2.144088  [ 6464/60000]\n","loss: 2.090827  [12864/60000]\n","loss: 2.107517  [19264/60000]\n","loss: 2.051038  [25664/60000]\n","loss: 2.002467  [32064/60000]\n","loss: 2.020483  [38464/60000]\n","loss: 1.944626  [44864/60000]\n","loss: 1.957958  [51264/60000]\n","loss: 1.879049  [57664/60000]\n","Test Error: \n"," Accuracy: 50.0%, Avg loss: 1.873589 \n","\n","Epoch 3\n","-------------------------------\n","loss: 1.918590  [   64/60000]\n","loss: 1.873711  [ 6464/60000]\n","loss: 1.763783  [12864/60000]\n","loss: 1.800990  [19264/60000]\n","loss: 1.692506  [25664/60000]\n","loss: 1.654520  [32064/60000]\n","loss: 1.668577  [38464/60000]\n","loss: 1.573558  [44864/60000]\n","loss: 1.608581  [51264/60000]\n","loss: 1.497404  [57664/60000]\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.510620 \n","\n","Epoch 4\n","-------------------------------\n","loss: 1.587590  [   64/60000]\n","loss: 1.537773  [ 6464/60000]\n","loss: 1.397743  [12864/60000]\n","loss: 1.466974  [19264/60000]\n","loss: 1.352543  [25664/60000]\n","loss: 1.353765  [32064/60000]\n","loss: 1.364027  [38464/60000]\n","loss: 1.286226  [44864/60000]\n","loss: 1.329328  [51264/60000]\n","loss: 1.232030  [57664/60000]\n","Test Error: \n"," Accuracy: 64.0%, Avg loss: 1.249897 \n","\n","Epoch 5\n","-------------------------------\n","loss: 1.333318  [   64/60000]\n","loss: 1.300255  [ 6464/60000]\n","loss: 1.146022  [12864/60000]\n","loss: 1.250435  [19264/60000]\n","loss: 1.130485  [25664/60000]\n","loss: 1.157886  [32064/60000]\n","loss: 1.175034  [38464/60000]\n","loss: 1.106535  [44864/60000]\n","loss: 1.154338  [51264/60000]\n","loss: 1.074886  [57664/60000]\n","Test Error: \n"," Accuracy: 65.3%, Avg loss: 1.086942 \n","\n","Epoch 6\n","-------------------------------\n","loss: 1.162978  [   64/60000]\n","loss: 1.149428  [ 6464/60000]\n","loss: 0.980588  [12864/60000]\n","loss: 1.113751  [19264/60000]\n","loss: 0.994405  [25664/60000]\n","loss: 1.026153  [32064/60000]\n","loss: 1.056915  [38464/60000]\n","loss: 0.991936  [44864/60000]\n","loss: 1.040050  [51264/60000]\n","loss: 0.974927  [57664/60000]\n","Test Error: \n"," Accuracy: 66.6%, Avg loss: 0.980926 \n","\n","Epoch 7\n","-------------------------------\n","loss: 1.044211  [   64/60000]\n","loss: 1.050669  [ 6464/60000]\n","loss: 0.866584  [12864/60000]\n","loss: 1.021703  [19264/60000]\n","loss: 0.908319  [25664/60000]\n","loss: 0.932839  [32064/60000]\n","loss: 0.979221  [38464/60000]\n","loss: 0.917058  [44864/60000]\n","loss: 0.960571  [51264/60000]\n","loss: 0.906862  [57664/60000]\n","Test Error: \n"," Accuracy: 67.8%, Avg loss: 0.908076 \n","\n","Epoch 8\n","-------------------------------\n","loss: 0.956838  [   64/60000]\n","loss: 0.981583  [ 6464/60000]\n","loss: 0.784836  [12864/60000]\n","loss: 0.955541  [19264/60000]\n","loss: 0.849785  [25664/60000]\n","loss: 0.864048  [32064/60000]\n","loss: 0.924499  [38464/60000]\n","loss: 0.866775  [44864/60000]\n","loss: 0.903364  [51264/60000]\n","loss: 0.857083  [57664/60000]\n","Test Error: \n"," Accuracy: 68.9%, Avg loss: 0.855280 \n","\n","Epoch 9\n","-------------------------------\n","loss: 0.889653  [   64/60000]\n","loss: 0.929828  [ 6464/60000]\n","loss: 0.723413  [12864/60000]\n","loss: 0.906160  [19264/60000]\n","loss: 0.807216  [25664/60000]\n","loss: 0.812175  [32064/60000]\n","loss: 0.883038  [38464/60000]\n","loss: 0.831843  [44864/60000]\n","loss: 0.860635  [51264/60000]\n","loss: 0.818616  [57664/60000]\n","Test Error: \n"," Accuracy: 70.0%, Avg loss: 0.815207 \n","\n","Epoch 10\n","-------------------------------\n","loss: 0.836023  [   64/60000]\n","loss: 0.888474  [ 6464/60000]\n","loss: 0.675460  [12864/60000]\n","loss: 0.867972  [19264/60000]\n","loss: 0.774492  [25664/60000]\n","loss: 0.772263  [32064/60000]\n","loss: 0.849587  [38464/60000]\n","loss: 0.806126  [44864/60000]\n","loss: 0.827594  [51264/60000]\n","loss: 0.787360  [57664/60000]\n","Test Error: \n"," Accuracy: 71.2%, Avg loss: 0.783370 \n","\n","Done!\n"]}]},{"cell_type":"markdown","source":["TEST"],"metadata":{"id":"qdD25QgG374a"}},{"cell_type":"code","source":["test_loss, correct = 0, 0\n","\n","for X, y in train_dataloader:\n","  pred = model(X)\n","  test_loss += loss_fn(pred, y).item()\n","  correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","  print(pred)\n","  # 최대값을 가지는 인덱스 반환\n","  print(pred.argmax(1))\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ySBJXMo63tcM","executionInfo":{"status":"ok","timestamp":1702533031032,"user_tz":-540,"elapsed":6,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"a02c01db-4e7a-4af9-98e4-75ec13dcb6d3"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-5.2892e+00, -8.5947e+00, -3.1694e+00, -3.0237e+00, -1.4874e+00,\n","          6.5159e+00, -2.6852e+00,  4.5896e+00,  4.6271e+00,  8.5195e+00],\n","        [ 7.6042e+00, -9.5918e-01,  2.6316e+00,  3.5016e+00,  1.0127e+00,\n","         -5.2327e+00,  4.6072e+00, -9.1702e+00, -4.4192e-01, -4.6143e+00],\n","        [ 1.4757e+00,  2.5883e+00,  2.5685e-01,  2.5484e+00,  8.7223e-01,\n","         -2.0115e+00,  9.1538e-01, -3.3640e+00, -1.6134e+00, -2.3397e+00],\n","        [ 3.5244e+00,  2.2243e+00,  1.6739e+00,  2.8681e+00,  1.4790e+00,\n","         -3.6319e+00,  2.4234e+00, -5.9630e+00, -1.6346e+00, -4.0305e+00],\n","        [ 3.0613e+00,  3.8625e+00, -1.8111e-01,  5.4474e+00,  1.3131e+00,\n","         -4.2237e+00,  1.5888e+00, -6.4711e+00, -1.8009e+00, -4.1080e+00],\n","        [ 1.9154e+00, -2.2373e+00,  5.3237e+00,  5.8161e-02,  4.1175e+00,\n","         -3.2682e+00,  4.2346e+00, -7.3644e+00,  8.4527e-01, -4.8341e+00],\n","        [-4.0816e+00, -4.4500e+00, -2.4722e+00, -1.7373e+00, -1.6095e+00,\n","          4.7541e+00, -2.1929e+00,  6.4334e+00,  2.4604e+00,  2.9988e+00],\n","        [ 4.2244e-01, -4.8440e+00,  5.9882e+00, -1.9219e+00,  5.1646e+00,\n","         -2.1863e+00,  4.3858e+00, -7.3313e+00,  2.6904e+00, -3.7229e+00],\n","        [-6.8439e-01, -1.7615e+00, -7.9602e-01, -4.4990e-01, -7.8359e-01,\n","          1.8601e+00, -5.4404e-01,  6.8069e-01,  7.1041e-01,  1.9364e+00],\n","        [-1.7696e+00, -4.3160e+00, -4.4315e-01, -1.9696e+00, -3.3029e-01,\n","          3.0480e+00, -4.8965e-01,  8.0548e-01,  2.4014e+00,  3.0356e+00],\n","        [ 6.6499e+00,  1.3402e+00,  2.1037e+00,  3.9827e+00,  1.0961e+00,\n","         -5.1627e+00,  3.7836e+00, -8.5700e+00, -1.7196e+00, -4.5744e+00],\n","        [-1.4793e+00, -5.3199e+00, -4.4261e-01, -5.6725e-01,  2.8581e-01,\n","          2.2329e+00, -1.7203e-01, -7.9090e-01,  1.6774e+00,  4.1807e+00],\n","        [-2.7341e+00, -3.1162e+00, -1.1908e+00, -1.3457e+00, -4.4104e-01,\n","          3.0397e+00, -1.1101e+00,  2.8940e+00,  1.7481e+00,  2.2761e+00],\n","        [-2.8032e+00, -2.7454e+00, -1.2033e+00, -1.4499e+00, -7.3579e-01,\n","          3.3328e+00, -1.2421e+00,  3.7060e+00,  1.5670e+00,  1.6076e+00],\n","        [-2.7234e+00, -2.5926e+00, -1.7668e+00, -1.0212e+00, -1.2467e+00,\n","          3.2506e+00, -1.5110e+00,  4.6907e+00,  1.3744e+00,  1.6690e+00],\n","        [-5.8145e+00, -8.0457e+00, -3.7144e+00, -3.2153e+00, -2.0589e+00,\n","          7.1225e+00, -3.3663e+00,  6.3507e+00,  4.0873e+00,  8.6492e+00],\n","        [ 2.3087e+00,  5.5815e+00,  3.6479e-01,  3.7312e+00,  1.6343e+00,\n","         -3.6252e+00,  1.1569e+00, -5.7959e+00, -2.9005e+00, -3.7049e+00],\n","        [ 6.7079e+00,  1.3832e+00,  2.3606e+00,  4.3335e+00,  1.8152e+00,\n","         -5.7671e+00,  4.2726e+00, -9.3396e+00, -1.6007e+00, -5.4982e+00],\n","        [ 1.5761e+00, -4.3244e+00,  5.1839e+00, -7.6101e-01,  4.2320e+00,\n","         -2.5910e+00,  4.5215e+00, -6.9098e+00,  2.4795e+00, -4.2928e+00],\n","        [ 1.8991e-01, -4.4110e-01,  1.5300e+00,  3.6600e-01,  1.8531e+00,\n","         -9.1572e-01,  1.4797e+00, -2.6507e+00,  5.5695e-02, -1.8988e+00],\n","        [ 4.2980e+00,  3.8404e+00,  7.4916e-02,  6.1163e+00,  7.3752e-01,\n","         -4.5816e+00,  1.9605e+00, -6.9684e+00, -2.8743e+00, -4.0087e+00],\n","        [ 1.4063e+00,  5.1083e+00,  4.7415e-02,  3.9037e+00,  1.7690e+00,\n","         -3.3053e+00,  8.9932e-01, -5.1734e+00, -2.3818e+00, -3.6328e+00],\n","        [ 1.5928e-01, -1.1321e+00,  9.7976e-01,  1.4379e+00,  2.5389e+00,\n","         -1.6207e+00,  1.6107e+00, -4.2779e+00,  1.1890e+00, -1.8302e+00],\n","        [-1.6633e+00, -7.1453e+00,  3.7337e-01, -2.7634e+00,  4.4429e-01,\n","          2.6131e+00,  6.7723e-01,  1.4644e-02,  5.6431e+00,  1.6953e+00],\n","        [ 1.4613e+00, -1.6330e+00,  5.5379e+00,  7.5604e-01,  6.1745e+00,\n","         -4.6026e+00,  5.0420e+00, -8.9389e+00,  1.0974e+00, -6.6048e+00],\n","        [ 2.5036e+00,  3.8838e+00,  8.5242e-01,  5.0718e+00,  3.0211e+00,\n","         -4.7092e+00,  2.2846e+00, -7.0967e+00, -2.3211e+00, -5.1023e+00],\n","        [ 5.3471e+00, -1.1358e+00,  2.7166e+00,  1.9793e+00,  1.1953e+00,\n","         -3.8852e+00,  3.9548e+00, -6.9768e+00,  5.3143e-01, -4.4879e+00],\n","        [ 2.6065e+00, -3.1216e+00,  6.9251e+00, -3.5596e-01,  5.7179e+00,\n","         -4.4531e+00,  5.5894e+00, -9.7398e+00,  1.3697e+00, -6.1195e+00],\n","        [ 1.1654e+00, -9.2426e-01,  3.9900e+00,  5.3032e-01,  4.1739e+00,\n","         -3.0462e+00,  3.4989e+00, -6.3040e+00,  2.0758e-01, -4.3685e+00],\n","        [ 2.0172e-01, -4.1245e+00,  4.9144e+00, -8.1656e-01,  5.5086e+00,\n","         -2.5947e+00,  4.3383e+00, -7.2118e+00,  2.7066e+00, -4.1517e+00],\n","        [-1.4049e+00, -1.5541e+00, -5.4096e-01, -5.6164e-01, -4.0368e-01,\n","          1.7358e+00, -4.4395e-01,  1.9700e+00,  1.1353e+00,  6.2379e-02],\n","        [ 1.7006e+00,  1.7050e+00, -5.2094e-01,  3.4101e+00,  3.6013e-01,\n","         -1.9619e+00,  7.9777e-01, -3.3542e+00, -8.3045e-01, -2.0835e+00],\n","        [ 2.0224e+00, -2.6701e+00,  3.8955e+00,  5.1125e-01,  3.9927e+00,\n","         -3.1093e+00,  4.1717e+00, -7.0447e+00,  1.4942e+00, -4.2896e+00],\n","        [ 2.4047e-01,  2.5831e-01,  9.9789e-01,  1.9496e-01,  1.1246e+00,\n","         -5.9180e-01,  9.5148e-01, -1.6964e+00, -3.1911e-01, -1.4807e+00],\n","        [ 1.7927e+00,  3.1164e-01,  9.1603e-01,  8.3975e-01,  6.0362e-01,\n","         -1.2560e+00,  1.3588e+00, -2.6596e+00, -3.4762e-01, -1.8961e+00],\n","        [-6.6927e-01, -3.1086e+00,  6.2457e-01, -3.6681e-01,  1.1317e+00,\n","          2.9992e-01,  1.0555e+00, -1.4864e+00,  3.0016e+00, -9.5634e-01],\n","        [ 9.1766e-01, -3.7791e+00,  4.9413e-02, -1.6058e-01, -1.9682e-01,\n","          6.1293e-01,  9.4173e-01, -1.8086e+00,  2.3998e+00,  9.3846e-01],\n","        [ 2.1630e+00, -1.5990e+00,  2.5758e+00,  1.6030e-01,  1.3705e+00,\n","         -1.6608e+00,  2.4351e+00, -4.3363e+00,  7.7916e-01, -2.4098e+00],\n","        [ 1.5289e+00,  9.0817e+00, -4.1383e-01,  5.2139e+00,  1.5648e+00,\n","         -4.2192e+00,  2.7439e-01, -6.0190e+00, -4.4798e+00, -4.4750e+00],\n","        [ 2.4989e+00, -2.4663e+00,  6.0091e+00,  1.1044e+00,  6.3241e+00,\n","         -5.2256e+00,  5.9479e+00, -1.0254e+01,  1.3358e+00, -7.0451e+00],\n","        [ 2.4326e+00, -1.4381e+00,  4.6871e+00,  1.3278e+00,  4.4548e+00,\n","         -4.1558e+00,  4.5883e+00, -8.2671e+00,  4.0538e-01, -5.4068e+00],\n","        [-4.3328e+00, -5.0897e+00, -3.0156e+00, -2.2048e+00, -2.1458e+00,\n","          5.4553e+00, -2.6543e+00,  6.9724e+00,  2.3340e+00,  4.7338e+00],\n","        [-5.6862e+00, -8.1411e+00, -3.8969e+00, -3.2981e+00, -2.3314e+00,\n","          7.2650e+00, -3.4669e+00,  6.9289e+00,  3.8148e+00,  8.8677e+00],\n","        [-3.2647e+00, -3.5245e+00, -1.5364e+00, -1.6008e+00, -8.3101e-01,\n","          3.8208e+00, -1.5134e+00,  3.9356e+00,  2.1553e+00,  2.3690e+00],\n","        [-3.1742e+00, -6.4691e+00, -1.5816e+00, -1.6516e+00, -7.5976e-01,\n","          4.6080e+00, -1.4261e+00,  1.4653e+00,  2.7336e+00,  5.9869e+00],\n","        [ 1.8847e+00, -2.1044e+00,  4.8293e+00,  7.9416e-02,  4.1403e+00,\n","         -3.3064e+00,  4.2521e+00, -7.0076e+00,  1.1282e+00, -4.9926e+00],\n","        [-4.2241e+00, -4.7294e+00, -2.3203e+00, -2.0361e+00, -1.4370e+00,\n","          4.8080e+00, -2.1211e+00,  6.1819e+00,  2.8654e+00,  2.9846e+00],\n","        [ 4.9337e+00,  3.9318e+00,  1.1691e+00,  6.2576e+00,  1.8751e+00,\n","         -5.8029e+00,  2.9771e+00, -8.7211e+00, -2.8480e+00, -5.5280e+00],\n","        [ 6.6500e+00, -4.3900e-01,  2.2078e+00,  2.9198e+00,  8.1215e-01,\n","         -4.4228e+00,  3.9006e+00, -7.8708e+00, -4.9739e-01, -4.0664e+00],\n","        [ 2.4955e+00,  1.0633e+00,  1.1890e+00,  4.5651e+00,  3.1578e+00,\n","         -4.2319e+00,  2.7689e+00, -7.0821e+00, -9.1050e-01, -4.5693e+00],\n","        [ 2.4513e+00,  4.7055e+00, -8.0534e-01,  6.3120e+00,  1.0630e+00,\n","         -3.9098e+00,  9.6529e-01, -5.8280e+00, -2.9602e+00, -3.4747e+00],\n","        [ 3.1464e+00,  4.5504e+00, -3.7907e-01,  5.7928e+00,  5.4078e-01,\n","         -3.9081e+00,  1.1424e+00, -5.7926e+00, -2.9765e+00, -3.5529e+00],\n","        [-4.4747e+00, -4.6767e+00, -2.3732e+00, -1.8643e+00, -1.3409e+00,\n","          4.8977e+00, -2.2254e+00,  6.3574e+00,  2.8170e+00,  2.9046e+00],\n","        [ 3.0988e+00, -2.8289e+00,  6.9479e+00,  7.2624e-01,  6.2966e+00,\n","         -5.4080e+00,  6.1194e+00, -1.1093e+01,  1.0152e+00, -6.7993e+00],\n","        [ 8.4065e-01, -2.8702e-01,  1.9500e+00,  5.3855e-01,  2.0907e+00,\n","         -1.6094e+00,  1.9553e+00, -3.6046e+00,  1.9244e-01, -2.6477e+00],\n","        [ 6.7979e+00, -1.6482e-01,  3.0416e+00,  3.2609e+00,  2.1731e+00,\n","         -5.5927e+00,  4.7350e+00, -9.7338e+00, -7.1382e-01, -5.0663e+00],\n","        [ 4.4615e+00,  3.0100e+00,  2.9045e+00,  3.5633e+00,  2.6137e+00,\n","         -5.0087e+00,  3.4719e+00, -7.8120e+00, -3.0664e+00, -5.6156e+00],\n","        [ 1.0395e+00, -2.3702e+00, -8.9688e-01,  1.6598e+00,  1.5282e-01,\n","         -4.7781e-01,  8.9453e-01, -1.7759e+00,  1.8532e+00, -5.4534e-01],\n","        [ 2.0400e+00,  4.4987e+00, -1.2401e+00,  6.2504e+00,  7.7816e-01,\n","         -3.5903e+00,  5.6277e-01, -5.2811e+00, -2.5027e+00, -3.0686e+00],\n","        [ 6.2309e-01,  3.5293e+00, -7.2476e-01,  4.3613e+00,  8.8498e-01,\n","         -2.1783e+00,  1.3840e-01, -3.4550e+00, -2.2740e+00, -2.0742e+00],\n","        [-1.9492e+00, -5.7005e+00, -3.7354e-01, -2.4099e+00, -1.4144e-01,\n","          3.3078e+00, -3.1658e-01,  4.9616e-01,  3.2282e+00,  3.9051e+00],\n","        [ 3.4855e+00, -8.4115e-01,  3.0838e+00,  1.2648e+00,  2.3136e+00,\n","         -3.3314e+00,  3.5629e+00, -6.3424e+00,  4.3537e-03, -4.1024e+00],\n","        [-1.8762e+00, -1.8409e+00, -5.3388e-01, -9.6666e-01, -2.5585e-01,\n","          2.2026e+00, -5.4872e-01,  2.3537e+00,  9.3704e-01,  5.6019e-01],\n","        [-4.6467e-01, -5.0830e-01, -2.0101e-01, -4.1695e-02, -1.7523e-01,\n","          7.3542e-01, -1.8784e-02,  6.0990e-01,  3.0201e-01, -2.7809e-01]],\n","       grad_fn=<AddmmBackward0>)\n","tensor([9, 0, 1, 0, 3, 2, 7, 2, 9, 5, 0, 9, 5, 7, 7, 9, 1, 0, 2, 4, 3, 1, 4, 8,\n","        4, 3, 0, 2, 4, 4, 7, 3, 6, 4, 0, 8, 8, 2, 1, 4, 2, 7, 9, 7, 9, 2, 7, 3,\n","        0, 3, 3, 3, 7, 2, 4, 0, 0, 8, 3, 3, 9, 6, 7, 5])\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"A5xFRr1A4F-F"},"execution_count":null,"outputs":[]}]}