{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMrW9Nvf3lJg6Fx8njC2ytw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## torch.nn.Module AND torch.nn.Parameter\n","##### 간단하게 두 개의 선형 레이어가 있는 간단한 모델을 만들어보자. 그리고 활성화 함수, 해당 인스턴스를 확인하고 매개변수를 보자."],"metadata":{"id":"ABWzQsKtqq2e"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fwDuHDappxj_","executionInfo":{"status":"ok","timestamp":1702882312063,"user_tz":-540,"elapsed":5019,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"dea7af8b-ef11-4d98-b89a-6262bf319fea"},"outputs":[{"output_type":"stream","name":"stdout","text":["The model:\n","TinyModel(\n","  (linear1): Linear(in_features=100, out_features=200, bias=True)\n","  (activation): ReLU()\n","  (linear2): Linear(in_features=200, out_features=10, bias=True)\n","  (softmax): Softmax(dim=None)\n",")\n","\n","\n","Just one layer:\n","Linear(in_features=200, out_features=10, bias=True)\n","\n","\n","Model params:\n","Parameter containing:\n","tensor([[-0.0374,  0.0263,  0.0571,  ..., -0.0867, -0.0959, -0.0104],\n","        [-0.0582, -0.0152, -0.0739,  ..., -0.0918, -0.0167,  0.0212],\n","        [-0.0754, -0.0262, -0.0191,  ...,  0.0883, -0.0620,  0.0323],\n","        ...,\n","        [-0.0636, -0.0395, -0.0972,  ..., -0.0005, -0.0262, -0.0414],\n","        [ 0.0947,  0.0294, -0.0543,  ...,  0.0540, -0.0505,  0.0392],\n","        [ 0.0173,  0.0258,  0.0978,  ...,  0.0816, -0.0123,  0.0325]],\n","       requires_grad=True)\n","Parameter containing:\n","tensor([ 0.0818,  0.0157, -0.0359,  0.0848, -0.0720,  0.0015,  0.0391, -0.0669,\n","        -0.0760,  0.0280,  0.0111,  0.0127, -0.0300, -0.0623, -0.0375, -0.0961,\n","         0.0116,  0.0589, -0.0826, -0.0955, -0.0237,  0.0546, -0.0658,  0.0828,\n","         0.0493, -0.0083,  0.0484,  0.0743,  0.0662, -0.0642,  0.0078, -0.0559,\n","         0.0159,  0.0567,  0.0965,  0.0116,  0.0541, -0.0120, -0.0512, -0.0873,\n","        -0.0515, -0.0556, -0.0966, -0.0448, -0.0027, -0.0768, -0.0699,  0.0487,\n","        -0.0278, -0.0521, -0.0565,  0.0854,  0.0677, -0.0299,  0.0001, -0.0542,\n","         0.0644,  0.0763, -0.0937, -0.0876,  0.0029, -0.0218,  0.0679,  0.0007,\n","         0.0205,  0.0181,  0.0971, -0.0091, -0.0167,  0.0845,  0.0358, -0.0706,\n","         0.0910,  0.0151,  0.0649, -0.0972,  0.0671,  0.0039,  0.0182, -0.0282,\n","         0.0612, -0.0135, -0.0905, -0.0672, -0.0954, -0.0202,  0.0552, -0.0119,\n","         0.0385,  0.0077, -0.0651, -0.0727, -0.0997,  0.0360, -0.0688,  0.0699,\n","        -0.0894, -0.0247, -0.0652, -0.0148, -0.0628,  0.0513, -0.0547,  0.0456,\n","         0.0143, -0.0423,  0.0171, -0.0898, -0.0869, -0.0590, -0.0554,  0.0929,\n","        -0.0407, -0.0834, -0.0250, -0.0498,  0.0032, -0.0166,  0.0719,  0.0406,\n","        -0.0027, -0.0887, -0.0144,  0.0985, -0.0478, -0.0675,  0.0340, -0.0878,\n","         0.0343, -0.0183, -0.0375, -0.0953, -0.0592,  0.0417,  0.0038, -0.0462,\n","         0.0624,  0.0977, -0.0160, -0.0906, -0.0563, -0.0541,  0.0185, -0.0393,\n","        -0.0373, -0.0539,  0.0333,  0.0562, -0.0025,  0.0673, -0.0761,  0.0822,\n","         0.0042, -0.0771, -0.0815, -0.0133,  0.0670, -0.0227, -0.0175, -0.0613,\n","         0.0449, -0.0426,  0.0401, -0.0003,  0.0996,  0.0203, -0.0806, -0.0944,\n","        -0.0780, -0.0530,  0.0469, -0.0937, -0.0227, -0.0168,  0.0602, -0.0582,\n","        -0.0744, -0.0213,  0.0437,  0.0574,  0.0916,  0.0834,  0.0261,  0.0212,\n","        -0.0927, -0.0704, -0.0647,  0.0557, -0.0404,  0.0152,  0.0273,  0.0837,\n","        -0.0804,  0.0808, -0.0944,  0.0165,  0.0748,  0.0014,  0.0628,  0.0853],\n","       requires_grad=True)\n","Parameter containing:\n","tensor([[-0.0153, -0.0023, -0.0621,  ..., -0.0222, -0.0656,  0.0011],\n","        [ 0.0179,  0.0700,  0.0088,  ..., -0.0118, -0.0667,  0.0107],\n","        [ 0.0515,  0.0608,  0.0063,  ..., -0.0075,  0.0236,  0.0704],\n","        ...,\n","        [ 0.0427, -0.0550,  0.0228,  ..., -0.0105,  0.0021,  0.0259],\n","        [-0.0026, -0.0695, -0.0106,  ...,  0.0565, -0.0388, -0.0385],\n","        [-0.0227,  0.0626,  0.0173,  ...,  0.0263,  0.0132, -0.0085]],\n","       requires_grad=True)\n","Parameter containing:\n","tensor([-0.0115,  0.0289, -0.0055,  0.0480, -0.0413, -0.0398,  0.0645, -0.0203,\n","         0.0679,  0.0191], requires_grad=True)\n","\n","\n","Layer params:\n","Parameter containing:\n","tensor([[-0.0153, -0.0023, -0.0621,  ..., -0.0222, -0.0656,  0.0011],\n","        [ 0.0179,  0.0700,  0.0088,  ..., -0.0118, -0.0667,  0.0107],\n","        [ 0.0515,  0.0608,  0.0063,  ..., -0.0075,  0.0236,  0.0704],\n","        ...,\n","        [ 0.0427, -0.0550,  0.0228,  ..., -0.0105,  0.0021,  0.0259],\n","        [-0.0026, -0.0695, -0.0106,  ...,  0.0565, -0.0388, -0.0385],\n","        [-0.0227,  0.0626,  0.0173,  ...,  0.0263,  0.0132, -0.0085]],\n","       requires_grad=True)\n","Parameter containing:\n","tensor([-0.0115,  0.0289, -0.0055,  0.0480, -0.0413, -0.0398,  0.0645, -0.0203,\n","         0.0679,  0.0191], requires_grad=True)\n"]}],"source":["import torch\n","\n","class TinyModel(torch.nn.Module):\n","\n","    def __init__(self):\n","        super(TinyModel, self).__init__()\n","        self.linear1 = torch.nn.Linear(100, 200)\n","        self.activation = torch.nn.ReLU()\n","        self.linear2 = torch.nn.Linear(200, 10)\n","        self.softmax = torch.nn.Softmax()\n","\n","    def forward(self, x):\n","        x = self.linear1(x)\n","        x = self.activation(x)\n","        x = self.linear2(x)\n","        x = self.softmax(x)\n","        return x\n","\n","tinymodel = TinyModel()\n","\n","print('The model:')\n","print(tinymodel)\n","\n","print('\\n\\nJust one layer:')\n","print(tinymodel.linear2)\n","\n","print('\\n\\nModel params:')\n","for param in tinymodel.parameters():\n","    print(param)\n","\n","print('\\n\\nLayer params:')\n","for param in tinymodel.linear2.parameters():\n","    print(param)"]},{"cell_type":"markdown","source":["## 공통 레이어 유형\n","### 선형 레이어\n","##### 가장 기본적인 유형의 신경망 계층은 linear 또는 fully connected 레이어이다. 모든 입력이 모든 요소에 영향을 미치는 레이어이다.\n","##### 만약 모델에 m 입력과 n 출력이 있는 경우 가중치는 m x n 행렬이다."],"metadata":{"id":"EHVHPPdLs2wx"}},{"cell_type":"code","source":["lin = torch.nn.Linear(3, 2)\n","x = torch.rand(1,3)\n","print('Input:')\n","print(x)\n","\n","print('\\n\\nWeight and Bias parameters:')\n","for param in lin.parameters():\n","  print(param)\n","\n","y = lin(x)\n","print('\\n\\nOutput:')\n","print(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_H0puCqTsbdR","executionInfo":{"status":"ok","timestamp":1702882852947,"user_tz":-540,"elapsed":7,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"b719236e-cfca-4499-922a-2065d5212616"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Input:\n","tensor([[0.3844, 0.9330, 0.1955]])\n","\n","\n","Weight and Bias parameters:\n","Parameter containing:\n","tensor([[-0.0963,  0.4044,  0.4871],\n","        [ 0.1712,  0.4085, -0.4989]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.0112, -0.4379], requires_grad=True)\n","\n","\n","Output:\n","tensor([[ 0.4467, -0.0884]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"markdown","source":["### 컨벌루션 레이어\n","##### conv layer는 높은 수준의 데이터를 처리하도록 구축되었다. (ex. 공간, nlp .. )"],"metadata":{"id":"3jwDYxOpum8R"}},{"cell_type":"code","source":["import torch.functional as F\n","\n","class LeNet(torch.nn.Module):\n","    def __init__(self):\n","        super(LeNet, self).__init__()\n","        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution\n","        # kernel\n","        self.conv1 = torch.nn.Conv2d(1, 6, 5)\n","        self.conv2 = torch.nn.Conv2d(6, 16, 3)\n","        # an affine operation: y = Wx + b\n","        self.fc1 = torch.nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n","        self.fc2 = torch.nn.Linear(120, 84)\n","        self.fc3 = torch.nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        # Max pooling over a (2, 2) window\n","        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n","        # If the size is a square you can only specify a single number\n","        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n","        x = x.view(-1, self.num_flat_features(x))\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","    def num_flat_features(self, x):\n","        size = x.size()[1:]  # all dimensions except the batch dimension\n","        num_features = 1\n","        for s in size:\n","            num_features *= s\n","        return num_features"],"metadata":{"id":"W6FxHszdthf_","executionInfo":{"status":"ok","timestamp":1702882955168,"user_tz":-540,"elapsed":5,"user":{"displayName":"천준석","userId":"06369588489203998973"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### 반복 레이어\n","##### 순환 신경망은 순차 데이터에 사용된다. 숨겨진 상태를 유지하여 해당 내용에 대한 일종의 메모리 역할을 한다.\n","##### RNN 레이어의 내부 구조 또는 그 변형인 LSTM을 테스트 해보자. -> LSTM 기반 품사 태거"],"metadata":{"id":"jTxOZQSSw2xr"}},{"cell_type":"code","source":["class LSTMTagger(torch.nn.Module):\n","    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n","        super(LSTMTagger, self).__init__()\n","        self.hidden_dim = hidden_dim\n","\n","        self.word_embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n","\n","        # The LSTM takes word embeddings as inputs, and outputs hidden states\n","        # with dimensionality hidden_dim.\n","        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n","\n","        # The linear layer that maps from hidden state space to tag space\n","        self.hidden2tag = torch.nn.Linear(hidden_dim, tagset_size)\n","\n","    def forward(self, sentence):\n","        embeds = self.word_embeddings(sentence)\n","        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n","        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n","        tag_scores = F.log_softmax(tag_space, dim=1)\n","        return tag_scores"],"metadata":{"id":"X7MhdWq_u6wh","executionInfo":{"status":"ok","timestamp":1702883668067,"user_tz":-540,"elapsed":296,"user":{"displayName":"천준석","userId":"06369588489203998973"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## 기타 레이어\n","### 데이터 조작 계층\n","##### 학습 과정에는 참여하지 않지만 데이터(텐서)를 조작하는 레이어이다."],"metadata":{"id":"2j-8rDx5xsY-"}},{"cell_type":"code","source":["my_tensor = torch.rand(1,6,6)\n","print(my_tensor)\n","\n","maxpool_layer = torch.nn.MaxPool2d(3)\n","print(maxpool_layer(my_tensor))\n","\n","# size가 pooling layer의 사이즈 만큼 줄어든다.\n","out = maxpool_layer(my_tensor)\n","print(out.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Edj1608pxqxX","executionInfo":{"status":"ok","timestamp":1702884475604,"user_tz":-540,"elapsed":354,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"66936fdc-3ccf-4d93-a710-fa380c24072a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[0.0447, 0.2731, 0.7717, 0.5675, 0.0985, 0.9274],\n","         [0.7809, 0.8720, 0.1859, 0.6821, 0.7895, 0.8516],\n","         [0.8636, 0.1887, 0.6128, 0.4421, 0.4876, 0.4232],\n","         [0.1133, 0.7171, 0.5039, 0.9983, 0.2850, 0.6433],\n","         [0.6282, 0.8662, 0.5032, 0.5585, 0.5198, 0.9074],\n","         [0.4858, 0.9407, 0.5888, 0.7439, 0.4679, 0.2335]]])\n","tensor([[[0.8720, 0.9274],\n","         [0.9407, 0.9983]]])\n","torch.Size([1, 2, 2])\n"]}]},{"cell_type":"markdown","source":["##### 각 사분면의 최대 값이다. (6 x 6)"],"metadata":{"id":"lXRM-4rXyFvu"}},{"cell_type":"markdown","source":["##### 정규화 레이어 한 레이어의 출력을 다시 중앙에 배치하고 정규화를 수행한다. 그러면 경사도 폭발/소멸 없이 학습률을 높일 수 있다."],"metadata":{"id":"ihkJjT43yPKC"}},{"cell_type":"code","source":["my_tensor = torch.rand(1, 4, 4) * 20 + 5\n","print(my_tensor)\n","print(my_tensor.mean())\n","\n","norm_layer = torch.nn.BatchNorm1d(4)\n","normed_tensor = norm_layer(my_tensor)\n","print(normed_tensor)\n","\n","print(normed_tensor.mean())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cnHMpzedx8aR","executionInfo":{"status":"ok","timestamp":1702883943964,"user_tz":-540,"elapsed":11,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"7f1ac033-ba88-4626-e892-3e2b4df30028"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[14.7523,  8.6027, 16.8663, 19.6900],\n","         [13.0565, 12.4996, 13.8683, 21.3891],\n","         [ 8.6157, 10.2775, 15.0089, 19.1571],\n","         [16.9850, 10.0303, 16.9798, 14.7375]]])\n","tensor(14.5323)\n","tensor([[[-0.0553, -1.5640,  0.4633,  1.1560],\n","         [-0.5956, -0.7501, -0.3704,  1.7162],\n","         [-1.1251, -0.7229,  0.4221,  1.4260],\n","         [ 0.8110, -1.6393,  0.8091,  0.0191]]],\n","       grad_fn=<NativeBatchNormBackward0>)\n","tensor(0., grad_fn=<MeanBackward0>)\n"]}]},{"cell_type":"markdown","source":["##### Dropout 레이어는 희소 표현을 장려한다. 즉, 축소된 데이터 세트에 대해 학습한다."],"metadata":{"id":"B45nXeydyz5R"}},{"cell_type":"code","source":["my_tensor = torch.rand(1, 4, 4)\n","\n","dropout = torch.nn.Dropout(p=0.4)\n","print(dropout(my_tensor))\n","print(dropout(my_tensor))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"daIkZmgVyuAL","executionInfo":{"status":"ok","timestamp":1702884049010,"user_tz":-540,"elapsed":379,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"8512594b-8d94-4aaa-e2a1-f9d2104c95a6"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[0.0000, 1.4565, 0.4831, 0.8894],\n","         [1.5687, 1.1433, 0.5709, 0.0000],\n","         [0.5527, 0.7883, 0.0000, 0.2089],\n","         [0.0310, 1.6056, 0.0036, 0.9695]]])\n","tensor([[[1.5673, 1.4565, 0.0000, 0.0000],\n","         [1.5687, 0.0000, 0.5709, 1.6012],\n","         [0.0000, 0.0000, 0.0000, 0.2089],\n","         [0.0310, 1.6056, 0.0000, 0.0000]]])\n"]}]},{"cell_type":"markdown","source":["### activation function\n","##### 이전 층의 결과값을 변환하여 다른 층의 뉴런으로 신호를 전달하는 역할을 한다. 모델의 복잡도를 올려 비선형 문제를 해결하는데 중요한 역할을 한다.\n","##### 단순 은닉층을 많이 쌓는다고 해서 비선형 문제를 해결할 순 없다. 활성화 함수를 사용하여 출력값이 linear하게 나오지 않게 하여 선형 분류기를 비선형 시스템으로 만든다.\n","##### 종류로는 sigmoid, ReLU, tanh 등이 있다."],"metadata":{"id":"W0o_If2lzSBC"}},{"cell_type":"markdown","source":["### loss function\n","##### 모델이 예측이 실제 값과 얼마나 떨어져 있는지 알 수 있다.\n","##### 종류로는 mse, crossentropy 등이 있다."],"metadata":{"id":"Z669A2EP0Nqt"}}]}